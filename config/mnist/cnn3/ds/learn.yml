criterion:
  name: ce

optimizer:
  name: adamw
  lr: 0.0001
  weight_decay: 0.0

scheduler:
  name: cosine
  lr_gamma: 0.99
  lr_min: 0.0

run:
  experiment: "mnist_cnn3_learn_ds" # data sampling
  n_epochs: 10
  batch_size: 32

quantize:
  weight_bit: 8

noise:
  phase_bias: 1
  phase_noise_std: 0
  gamma_noise_std: 0.002
  crosstalk_factor: 0.005
  random_state: 42

sl: # subspace learning
  noisy_identity: 0

sparse:
  bp_data_sparsity: 0
  bp_data_alg: smb # smd: sto. mini-batch drop; smb: standard mini-batch; is: importance sampling

checkpoint:
  save_best_model_k: 3
  checkpoint_dir: "mnist/cnn3/learn_ds"
  model_comment: ""
  resume: 0
  restore_checkpoint: ./checkpoint/mnist/cnn3/pm/SparseBP_MZI_CNN_wb-8_ib-32_icalg-zcd_icadapt-0_icbest-1__acc-94.66_epoch-300.pt
  restore_checkpoint_pretrained: "" #"./checkpoint/fmnist/cnn/pretrain/SparseBP_MZI_CNN_wb-32_ib-32__acc-92.66_epoch-79.pt"
  no_linear: 0

model:
  name: "SparseBP_MZI_CNN"
  mode: "usv"

debug:
  verbose: 1

